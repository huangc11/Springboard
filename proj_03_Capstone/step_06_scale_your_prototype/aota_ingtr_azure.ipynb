{"cells":[{"cell_type":"code","source":["c_fail = -1\nc_succ = 1\nc_ff_num_partition=8\nc_proc_name = 'fact-table-generation'\n\n\nclass CustomError(Exception):\n    pass\n\n\nfrom pyspark.sql import functions as f\n\nfrom pyspark.sql.functions  import col\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9309b5e-f730-4ddd-ad49-f05837f11820"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\nfrom pyspark.sql import SparkSession\nspark = SparkSession. \\\n                builder. \\\n                appName(\"Aggregation\"). \\\n                master(\"local[4]\"). \\\n                getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f929a021-9a50-44d9-9797-7311dfa2af10"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\n      \n\nclass ut_store:\n    '''place holder for  parameters'''\n\n    sparksql_shuffle_partition_num = 4\n\n    c_base_folder   = '/mnt/aota/'\n    c_fold_sep      = '/'\n\n\n\n    folder_raw= c_base_folder + 'raw/'\n    folder_cln= c_base_folder + 'processed/'\n    folder_intgr= c_base_folder + 'integrated/'\n    folder_log =  c_base_folder + 'logs/'\n\n    \n\n    #Raw files name\n    raw_file_plane = 'plane.csv'\n    raw_file_flight ='2008.csv.bz2'\n    raw_file_carrier = 'carrier.csv'\n    raw_file_airport = 'airport.csv'\n\n    #Processed/Clean file name\n    cln_file_plane ='plane'\n    cln_file_flight = 'flight'\n    cln_file_carrier = 'carrier'\n    cln_file_airport = 'airport'\n\n    # dimension table data files\n    file_dim_plane ='dim_plane'\n    file_dim_date ='dim_date'\n    file_dim_carrier='dim_carrier'\n    file_dim_origin_dest = 'dim_origin_dest'\n\n    # fact table data file\n    file_fact_flight ='fact_flight'\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07012ebe-1acb-4589-abd3-8f29f69e4237"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\n\nfrom pyspark.sql.types import \\\n        StructType,\\\n        StringType, \\\n        IntegerType, \\\n        DateType, \\\n        TimestampType,\\\n        FloatType,\\\n        BooleanType\n\nclass ut_spark:\n    \"\"\" a class of utitlies related to Spark, including tools to:\n    1. get spark session\n    2. read from/write to files\n    3. translate atrribute list to spark schema\n\n\n\n    \"\"\"\n\n    c_succ = True\n    c_fail = False\n    c_spark_sql_shuffle_partitions = ut_store.sparksql_shuffle_partition_num\n\n    __spark =None\n\n    @staticmethod\n    def isFile(f_path):\n        # Checking if a file exists with Pathlib\n        file_path = Path(f_path)\n        return file_path.is_file()\n\n    @classmethod\n    def log1_disp_error(cls, msg):\n        print(msg)\n\n\n\n    '''***********************************************************************\n\n    Create Spark / Get spark session /Init\n\n    ***********************************************************************'''\n\n    @classmethod\n    def creat_spark(cls):\n        '''Create and return a spark session'''\n\n        try:\n            spark = SparkSession. \\\n                builder. \\\n                appName(\"Aggregation\"). \\\n                master(\"local[4]\"). \\\n                getOrCreate()\n\n            spark.conf.set(\"spark.sql.shuffle.partitions\", cls.c_spark_sql_shuffle_partitions)\n\n            return spark\n\n        except Exception as e:\n            ut_log.log_exeption(e)\n            return None\n\n\n    @staticmethod\n    def get_sparksession():\n        if ut_spark.__spark==None:\n            ut_spark.__spark=ut_spark.creat_spark()\n\n\n\n        return ut_spark.__spark\n\n    @classmethod\n    def  initialise(cls):\n        if cls.__spark == None:\n         ut_spark.__spark = ut_spark.creat_spark()\n\n\n\n\n    '''***********************************************************************\n    \n    Read from /Write to files\n    \n    ***********************************************************************'''\n\n    @classmethod\n    def df_read_from_csv(cls, file_path, schema=None):\n        ''' Create a dataframe from a csv file,with header, with/without schema'''\n\n        msg_fail = 'Spark-read-from-csv2 of {} failed --File does not exist!'.format((file_path))\n\n        if cls.isFile(file_path) != True:\n            msg_fail_on_file = 'Spark-read-from-csv1 of {} failed --File does not exist!'.format((file_path))\n            ut_log.log_error(msg_fail_on_file)\n            return None\n\n        try:\n            spark = cls.get_sparksession()\n            if schema==None:\n                  try:\n                      return spark.read\\\n                       .option(\"header\",\"true\") \\\n                        .option(\"inferSchema\",\"true\") \\\n                        .csv(file_path)\n                  except Exception as e:\n                      ut_log.log_error(msg_fail)\n                      ut_log.log_exeption(e)\n                      return None\n            else:\n                  try:\n                      return spark.read \\\n                          .option(\"header\", \"true\") \\\n                          .schema(schema) \\\n                          .csv(file_path)\n                  except:\n                      ut_log.log_error(msg_fail)\n                      return None\n        except Exception as e:\n            ut_log.log_error(msg_fail)\n            ut_log.log_exeption(e)\n            return None\n\n\n\n    @classmethod\n    def df_read_from_parquet(cls, file_path):\n\n       try:\n          spark = ut_spark.get_sparksession()\n          df=  spark.read.parquet(file_path)\n          return df\n       except Exception as e:\n           ut_log.log_exeption(e)\n           return None\n\n\n\n    @staticmethod\n    def df_write_to_file(df, file_path, format=\"parquet\"):\n\n        msg_write_fail = \"Writing file (file={}, format={}) failed.\".format(file_path, format)\n\n        if format not in (\"csv\",\"parquet\",\"json\"):\n            ut_spark.log_disp_error(\"Invalide format. Action failed.\")\n            return ut_spark.c_fail\n\n        try:\n            df.write \\\n                .format(format) \\\n                .mode(\"overwrite\") \\\n                .option(\"path\", file_path) \\\n                .save()\n\n            return ut_spark.c_succ\n        except Exception as e:\n            print(e)\n            ut_log.log_error(msg_write_fail)\n            ut_log.log_exeption(e)\n            return ut_spark.c_fail\n\n\n    '''***********************************************************************\n    \n    Schema  Translation\n    \n    ***********************************************************************'''\n\n\n    @classmethod\n    def list_to_schema(cls, fieldlist):\n        def get_datatype(field_type):\n            f_type = field_type[0:3]\n            if f_type == 'str':\n                return StringType()\n            elif f_type == 'int':\n                return IntegerType()\n            elif f_type == \"dat\":\n                return DateType()\n            elif f_type == \"boo\":\n                return BooleanType()\n            elif f_type == \"flo\":\n                return FloatType()\n            else:\n                return StringType()\n\n        structSchema = StructType()\n\n        for fieldTuple in fieldlist:\n            structSchema.add(fieldTuple[0], get_datatype(fieldTuple[1]), True)\n\n        return (structSchema)\n\n\n    '''***********************************************************************\n    \n    Other tools\n    ***********************************************************************'''\n\n    @staticmethod\n    def df_cr_tempview(df, table_name):\n        try:\n            df.createOrReplaceTempView(table_name)\n        except Exception as e:\n            ut_log.log_exeption(e)\n            ut_log.log_warning(('Dataframe ({}) temp view creation failed.').format(df))\n\n\n\n    @staticmethod\n    def show_partition_count(df):\n        df.groupBy(f.spark_partition_id()).count().show()\n\n    @staticmethod\n    def add_col(df, new_col, exp):\n        '''Add a  monotonically increasing id column to a dataframe'''\n        df1 = df.withColumn(new_col, exp)\n        return df1\n\n    @staticmethod\n    def add_mono_incr_id(df, new_col):\n        '''Add a  monotonically increasing id column to a dataframe'''\n        df1 = df.withColumn(new_col, f.monotonically_increasing_id())\n        return df1\n\n\n    @staticmethod\n    def df_dup_by_col(df, colname):\n        return df.groupBy(col(colname)).agg(count(colname).alias(\"count\")).filter(col(\"count\") > 1)\n\n\n    @staticmethod\n    def get_partition_num(df):\n        ''' Get number of partitions of a dataframe\n        '''\n        try:\n            n = df.rdd.getNumPartitions()\n            return n\n        except Exception as e:\n            ut_log.log_exeption(e)\n            return None\n\n\nif __name__ == \"__main__\":\n    pass\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"766cd0c7-dad4-4887-ab9d-13d0ae72f491"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class ut_log:\n   def  log_info(msg):\n      print(msg)\n      \n   def log_exeption(msg):\n      print(msg)\n      \n                  \n   def  log_proc_abort(c_proc_name):\n     pass\n    \n   def  log_proc_metric(name):\n      pass\n      \n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89a9a084-4959-433a-8773-5db8956855ae"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import time\nimport contextlib\n\nfrom datetime import  datetime\n\n\nfrom pathlib import Path\n\nclass ut_base:\n    const_succ = 1\n    const_fail = -1\n    date_format = \"%m/%d/%Y-%H:%M:%S\"\n    F_FAIL = -1\n\n    '''****************************************************************\n            Time/Date related methods\n    ********************************************************************'''\n\n    @staticmethod\n    def curr_time():\n      return datetime.now()\n\n    @staticmethod\n    def curr_date():\n      return datetime.today()\n\n    @staticmethod\n    def curr_time_str():\n      ''' Get a string out of current timestamp. Format: 20220321_120301'''\n      return ut_base.time_to_str(datetime.now(),\"%Y%m%d_%H%M%S\")\n\n    @staticmethod\n    def curr_time_fmt():\n      ''' Get a string out of current timestamp. Format: 2022-03-21_12:03:01'''\n\n      return ut_base.time_to_str(datetime.now(),\"%Y-%m-%d_%H:%M:%S\")\n\n\n    @staticmethod\n    def time_to_str(p_datetime, p_dt_format=None):\n      '''To conver a timestamp to a string.  Format can vary or be empty.'''\n      if p_dt_format ==None:\n         p_dt_format = ut_base.date_format\n\n      return p_datetime.strftime(p_dt_format)\n\n    @staticmethod\n    def str_to_time(p_dt_string, p_dt_format=None):\n      '''To convert a string to a timestamp .  Format can vary or be empty.'''\n      if p_dt_format ==None:\n         p_dt_format =ut_base.date_format\n      return datetime.strptime(p_dt_string, p_dt_format)\n\n\n\n    @staticmethod\n    @contextlib.contextmanager\n    def timer():\n      \"\"\"Time the execution of a context block.\n\n      Yields:\n        None\n      \"\"\"\n      start = time.time()\n      # Send control back to the context block\n      yield\n      end = time.time()\n      #print('Elapsed: {:.2f}s'.format(end - start))\n      print('Elapsed:%6f'%(end - start))\n\n\n    '''****************************************************************\n            Methods to pring dict, list and check file path\n    ********************************************************************'''\n\n    @staticmethod\n    def isFile(f_path):\n      # Checking if a file exists with Pathlib\n      file_path = Path(f_path)\n      return file_path.is_file()\n\n\n    def print_dict(p_dict):\n      line1 = ''\n      for key in p_dict:\n        line1 = line1 + '{}:{},'.format(key, p_dict[key])\n\n      print(line1)\n\n    def print_dict_val(p_dict:dict):\n      line1 = ''\n      for key in p_dict:\n        line1 = line1 + '{},'.format(p_dict[key])\n      print(line1)\n\n\n\n    @staticmethod\n    def print_list(list:list):\n      '''Print a list object'''\n      [print(l) for l in list]\n\n\n    @staticmethod\n    def print_dict_v(p_dict):\n          '''Print  all the values of a dict object'''\n          line1=''\n          for key in p_dict:\n              line1=line1+'{},'.format(p_dict[key])\n          print(line1)\n\n    def print_dict(p_dict):\n          '''Print out all the keys of a dict object '''\n          line1=''\n          for key in p_dict:\n              line1=line1+'{}:{},'.format(key,p_dict[key])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ceb7b25d-17ad-4b55-86d1-fde7155c6998"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def gen_date_key( year, month, day):\n        sep = ''\n        return str(year) + sep + str(month).rjust(2, '0') + sep + str(day).rjust(2, '0')\n    \ndef gen_orig_dest_key(origin,dest):\n        sep = '_'\n        return origin + sep + dest\n\n\n\n        c_proc_name= 'fact-table-generation'\n        udf_gen_date_key = f.udf(gen_date_key, returnType=f.StringType())\n        udf_gen_orig_dest_key = f.udf(gen_orig_dest_key, returnType=f.StringType())\n\n        rfp=ut_store.folder_cln + ut_store.cln_file_flight \n        #+'/year=1998'\n        print(rfp)\n\n        df_fl = ut_spark.df_read_from_parquet(rfp)  \n\n\n        # Adding date_key for buildinng dim_date later\n        df_fl1 = df_fl.withColumn(\"date_id\", udf_gen_date_key(\"Year\", \"Month\", \"DayofMonth\"))\n\n        # Adding orig_dest_key for buildinng dim_orign_dest\n        df_fl2 = df_fl1.withColumn(\"orig_dest_key\", udf_gen_orig_dest_key(\"Dest\", \"Origin\"))\n\n        df_fl_base = ut_spark.add_mono_incr_id(df_fl2, \"seq_id\")\n        df_fl_base.groupBy(\"year\").count().show()\n        \n        return df_fl_base\n        "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e546ad6-f8f3-4e73-9dfd-9ee48c673ea8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\n        c_proc_name= 'fact-table-generation'\n        udf_gen_date_key = f.udf(gen_date_key, returnType=f.StringType())\n        udf_gen_orig_dest_key = f.udf(gen_orig_dest_key, returnType=f.StringType())\n\n        rfp=ut_store.folder_cln + ut_store.cln_file_flight \n        #+'/year=1998'\n        print(rfp)\n\n        df_fl = spark.read.parquet(rfp)  \n\n\n       "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75ab31a2-1d6e-4368-a3cb-6b0bf54b60b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/mnt/aota/processed/flight\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/mnt/aota/processed/flight\n"]}}],"execution_count":0},{"cell_type":"code","source":["\n        # Adding date_key for buildinng dim_date later\n        df_fl1 = df_fl.withColumn(\"date_id\", udf_gen_date_key(\"Year\", \"Month\", \"DayofMonth\"))\n\n        # Adding orig_dest_key for buildinng dim_orign_dest\n        df_fl2 = df_fl1.withColumn(\"orig_dest_key\", udf_gen_orig_dest_key(\"Dest\", \"Origin\"))\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f055724-04e7-498f-afd0-53e7445d77bd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["        df_fl_base = ut_spark.add_mono_incr_id(df_fl2, \"seq_id\")\n        df_fl_base.groupBy(\"year\").count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e018e856-1b01-4ab1-b6f5-80c8f9f65cf6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----+-------+\n|year|  count|\n+----+-------+\n|2006|7003802|\n|2004|6987729|\n|2005|6992838|\n|2003|6375690|\n|2002|5197860|\n|2001|5723673|\n|1999|5360018|\n|2000|5481303|\n|1997|5301999|\n|1998|5227051|\n+----+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----+-------+\n|year|  count|\n+----+-------+\n|2006|7003802|\n|2004|6987729|\n|2005|6992838|\n|2003|6375690|\n|2002|5197860|\n|2001|5723673|\n|1999|5360018|\n|2000|5481303|\n|1997|5301999|\n|1998|5227051|\n+----+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["''' **************************************************\n              Create dimension table -- dim_date\n   *****************************************************'''\ndf_date = df_fl_base.select(\"date_id\", \"Year\", \"Month\", \"DayofMonth\", \"DayOfWeek\").distinct()\nres = ut_spark.df_write_to_file(df_date, ut_store.folder_intgr + \"dim_date\", format=\"parquet\")\nif res ==ut_spark.c_fail:\n    raise CustomError\nut_log.log_info(\"dim_date created.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f021b5f-51fd-4148-9d11-bd6820de910b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"dim_date created.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["dim_date created.\n"]}}],"execution_count":0},{"cell_type":"code","source":["df_date.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a74e596-8702-4f7e-9636-2874798516c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[61]: 3652","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[61]: 3652"]}}],"execution_count":0},{"cell_type":"code","source":["''' **************************************************\n  Create dimension table -- dim_orig_dest\n*****************************************************'''\n\n\ndf_od1 = df_fl_base.select(\"orig_dest_key\", \"origin\", \"dest\", \"distance\").distinct()\ndf_ap = ut_spark.df_read_from_parquet(ut_store.folder_cln + 'airport')\n\ndf_od2 = df_od1.join(df_ap, df_od1[\"Origin\"] == df_ap[\"iata\"]). \\\n    select(col(\"orig_dest_key\")\n           , col(\"origin\")\n           , col(\"dest\")\n           , col(\"distance\")\n           , col(\"airport\").alias(\"orig_airport\")\n           , col(\"city\").alias(\"orig_city\")\n           , col(\"state\").alias(\"orig_state\")\n           , col(\"country\").alias(\"orig_country\")\n           )\n\ndf_od3 = df_od2.join(df_ap, df_od1[\"Dest\"] == df_ap[\"iata\"]). \\\n    select(col(\"orig_dest_key\").alias(\"orig_dest_id\")\n           , col(\"origin\")\n           , col(\"dest\")\n           , col(\"distance\")\n           , col(\"orig_airport\")\n           , col(\"orig_city\")\n           , col(\"orig_state\")\n           , col(\"orig_country\")\n           , col(\"airport\").alias(\"dest_airport\")\n           , col(\"city\").alias(\"dest_city\")\n           , col(\"state\").alias(\"dest_state\")\n           , col(\"country\").alias(\"dest_country\")\n           )\n\nres = ut_spark.df_write_to_file(df_od3, ut_store.folder_intgr + \"dim_origin_dest\", format=\"parquet\")\nif res ==ut_spark.c_fail:\n    raise CustomError\nut_log.log_info(\"dim_orig_dest created.\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bd8ef76-1c0e-4ff0-9192-a3940381b5db"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"dim_orig_dest created.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["dim_orig_dest created.\n"]}}],"execution_count":0},{"cell_type":"code","source":["\n\n''' **************************************************\n  Create fact table -- fact_flight\n*****************************************************'''\ndf_fact_flt =df_fl_base.select\\\n        (col(\"year\")\n        , col(\"month\")\n        , col(\"seq_id\")\n        , col(\"date_id\")\n        , col(\"orig_dest_key\").alias(\"orig_dest_id\")\n        , col(\"flightNum\")\n        , col(\"uniqueCarrier\").alias(\"carrier_id\")\n        , col(\"tailNum\").alias(\"plane_id\")\n        , col(\"DepTime\")\n        , col(\"CRSDepTime\").alias(\"scheduled_deptime\")\n        , col(\"ArrTime\")\n        , col(\"CRSArrTime\").alias(\"scheduled_arrtime\")\n        , col(\"ArrDelay\")\n        , col(\"DepDelay\")\n        )\n\nut_log.log_info('df_fact_flt created.')\ndf_fact_flt.groupBy(\"year\").count().show()\ndf_fact_flt.printSchema()\n\n# partition the fact dataset by \"seq_id\" (using seq_id to average the size of each partition)\ndf_fact_flt_repart=df_fact_flt.repartition(c_ff_num_partition, \"seq_id\")\nut_spark.show_partition_count(df_fact_flt_repart)\n\n'''-------------------------------------------------------\nWrite fact table to storing location (\"integeratio\" layer)\n--------------------------------------------------------'''\n\n#get file path\nwf_path = ut_store.folder_intgr + ut_store.file_fact_flight\n\n#Process by year: collect year information to df2\ndf2 = df_fact_flt_repart.groupBy(\"year\").count().orderBy(\"year\")\n\ndataCollect = df2.collect()\nfor row in dataCollect:\n    year = row['year']\n\n    df3 = df_fact_flt_repart.filter(f.col(\"year\") == year)\n    print('year={}:count={}'.format(year, df3.count()))\n    #df3.write.format(\"parquet\") \\\n    df3.write.format(\"parquet\") \\\n        .mode(\"append\") \\\n        .partitionBy(\"year\") \\\n        .option(\"path\", wf_path) \\\n        .save()\n\nut_log.log_info(\"fact_flight created.\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed6b20da-cfd4-465b-b52c-f2002dd364dc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"df_fact_flt created.\n+----+-------+\n|year|  count|\n+----+-------+\n|2006|7003802|\n|2004|6987729|\n|2005|6992838|\n|2003|6375690|\n|2002|5197860|\n|2001|5723673|\n|1999|5360018|\n|2000|5481303|\n|1997|5301999|\n|1998|5227051|\n+----+-------+\n\nroot\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- seq_id: long (nullable = false)\n |-- date_id: string (nullable = true)\n |-- orig_dest_id: string (nullable = true)\n |-- flightNum: string (nullable = true)\n |-- carrier_id: string (nullable = true)\n |-- plane_id: string (nullable = true)\n |-- DepTime: string (nullable = true)\n |-- scheduled_deptime: string (nullable = true)\n |-- ArrTime: string (nullable = true)\n |-- scheduled_arrtime: string (nullable = true)\n |-- ArrDelay: integer (nullable = true)\n |-- DepDelay: integer (nullable = true)\n\n+--------------------+-------+\n|SPARK_PARTITION_ID()|  count|\n+--------------------+-------+\n|                   0|7454912|\n|                   1|7460719|\n|                   2|7458282|\n|                   3|7452707|\n|                   4|7458495|\n|                   5|7458022|\n|                   6|7452914|\n|                   7|7455912|\n+--------------------+-------+\n\nyear=1997:count=5301999\nyear=1998:count=5227051\nyear=1999:count=5360018\nyear=2000:count=5481303\nyear=2001:count=5723673\nyear=2002:count=5197860\nyear=2003:count=6375690\nyear=2004:count=6987729\nyear=2005:count=6992838\nyear=2006:count=7003802\nfact_flight created.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["df_fact_flt created.\n+----+-------+\n|year|  count|\n+----+-------+\n|2006|7003802|\n|2004|6987729|\n|2005|6992838|\n|2003|6375690|\n|2002|5197860|\n|2001|5723673|\n|1999|5360018|\n|2000|5481303|\n|1997|5301999|\n|1998|5227051|\n+----+-------+\n\nroot\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- seq_id: long (nullable = false)\n |-- date_id: string (nullable = true)\n |-- orig_dest_id: string (nullable = true)\n |-- flightNum: string (nullable = true)\n |-- carrier_id: string (nullable = true)\n |-- plane_id: string (nullable = true)\n |-- DepTime: string (nullable = true)\n |-- scheduled_deptime: string (nullable = true)\n |-- ArrTime: string (nullable = true)\n |-- scheduled_arrtime: string (nullable = true)\n |-- ArrDelay: integer (nullable = true)\n |-- DepDelay: integer (nullable = true)\n\n+--------------------+-------+\n|SPARK_PARTITION_ID()|  count|\n+--------------------+-------+\n|                   0|7454912|\n|                   1|7460719|\n|                   2|7458282|\n|                   3|7452707|\n|                   4|7458495|\n|                   5|7458022|\n|                   6|7452914|\n|                   7|7455912|\n+--------------------+-------+\n\nyear=1997:count=5301999\nyear=1998:count=5227051\nyear=1999:count=5360018\nyear=2000:count=5481303\nyear=2001:count=5723673\nyear=2002:count=5197860\nyear=2003:count=6375690\nyear=2004:count=6987729\nyear=2005:count=6992838\nyear=2006:count=7003802\nfact_flight created.\n"]}}],"execution_count":0},{"cell_type":"code","source":["\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19474179-67b9-4175-bfd5-3b34c0624029"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"183c6238-d7eb-450d-9c02-3844fb87269f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7773216e-4139-46a4-be8d-cb2ea4fcb554"}},"outputs":[],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.8.8","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"aota_ingtr_azure","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":174380401312357}},"nbformat":4,"nbformat_minor":0}
